{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3764ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from drawing import Drawing\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ddd3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing_custom_number(preprocess, filepath = './figs', return_img=True):\n",
    "    if (not os.path.isdir('figs'))and (filepath == './figs'):\n",
    "        os.mkdir('figs')\n",
    "    draw = Drawing()\n",
    "    draw.main(preprocess=preprocess,filepath=filepath)\n",
    "    img = Image.open(draw.file)\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.show()\n",
    "    if return_img:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4abd161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved ./figs/7.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = drawing_custom_number(preprocess=True,return_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95db108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f41c4b6be70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22aa4140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ead005c050b4b628412cb0aed84089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf86ac3d4544039a02ec7ea23fef7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe61b355d574b939f350a9f74f98605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c3f9589cf44e1c8b4668ca2babd0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labina/anaconda3/envs/NLB/lib/python3.8/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9143129ccd50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#just to check the datas/ among all the dataset shows selected data randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrandom_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtarget_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Target:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root ='./data',\n",
    "                              train =True, \n",
    "                              download=True,\n",
    "                              transform = transforms.ToTensor()) # A function that takes in an PIL image and returns a transformed version\n",
    "test_dataset = datasets.MNIST(root = './data',\n",
    "                             train=False,\n",
    "                             transform = transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53cb101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n",
      "Size of image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#just to check the datas/ among all the dataset shows selected data randomly\n",
    "idx = torch.randint(0, len(train_dataset), (1,)).item()\n",
    "random_image = train_dataset[idx][0].squeeze().numpy()\n",
    "target_num = train_dataset[idx][1]\n",
    "print('Target:'.format(target_num))\n",
    "print('Size of image:'.format(random_image.shape))\n",
    "\n",
    "plt.imshow(random_image, cmap = 'gray')\n",
    "plt.axis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad81fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#minibatch size \n",
    "batch = 64\n",
    "device = 'cuda' if torch.cuda.is_available()else 'cpu'\n",
    "print(device)\n",
    "#total epoch \n",
    "STEP = 10 \n",
    "#train and test loader\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                         batch_size = batch,\n",
    "                         shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                        batch_size = batch,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cc3ce",
   "metadata": {},
   "source": [
    "### to check total size of data taken in one iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b86772ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for (data,target) in train_loader:\n",
    "    print(data.size(),target.size())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5969106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=128)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "\n",
    "        self.out_fc = nn.Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        output = self.out_fc(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5cffdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_size =(28,28), num_classes = 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "900d7def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Net(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (out_fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a11309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:109386\n"
     ]
    }
   ],
   "source": [
    "#loss function and optimizer \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#calculating the number of parameters\n",
    "num_params = 0\n",
    "for params in model.parameters():\n",
    "    num_params += params.view(-1).size(0)\n",
    "print('Total number of parameters:{}' .format(num_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5f3a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,loss_func,optimizer,step,device,print_step=200):\n",
    "    '''Train Function'''\n",
    "    model.train()\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #to print the middle steps\n",
    "        if batch_idx % print_step == 0:\n",
    "            print('Train Step: {} ({:05.2f}%)  \\tLoss: {:.4f}'.format(\n",
    "                step, 100.*(batch_idx*train_loader.batch_size)/len(train_loader.dataset), \n",
    "                loss.item()))\n",
    "def test(model,test_loader,loss_func,device):\n",
    "    '''test function'''\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            #calculate loss \n",
    "            test_loss += loss_func(output,target,reduction=\"sum\").item()\n",
    "            pred = output.softmax(1).argmax(dim=1,keepdim=True)\n",
    "            #calculate the accurate prediction\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss/= len(test_loader.dataset)\n",
    "    test_acc = correct/len(test_loader.dataset)\n",
    "    print('Test Set: Average loss:{:.4f},Accuracy:{}/{}({:05.2f}%)'.format(\n",
    "    test_loss,correct,len(test_loader.dataset),100.*test_acc))\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def main(model,train_loader,test_loader,loss_func,optimizer,n_step,device,save_path,print_step):\n",
    "    test_accs = []\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for step in range(1,n_step+1):\n",
    "        #training \n",
    "        train(model,train_loader,loss_func, optimizer,\n",
    "             step=step,device=device,print_step=print_step)\n",
    "        #evaluation\n",
    "        test_loss, test_acc = test(model,test_loader,loss_func=F.cross_entropy,device=device)\n",
    "        \n",
    "        #to keep record of accuracy \n",
    "        test_accs.append(test_acc)\n",
    "        #to decide wether to save the optimal parameter/test results or not \n",
    "        if len(test_accs)>=2:\n",
    "            if test_acc >= best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_state_dict = model.state_dict()\n",
    "                print(\"discard previous state, best model state saved!\")\n",
    "        print(\"\")\n",
    "    torch.save(best_state_dict,save_path)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4adbe773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 1 (00.00%)  \tLoss: 0.0595\n",
      "Train Step: 1 (21.33%)  \tLoss: 0.0523\n",
      "Train Step: 1 (42.67%)  \tLoss: 0.0689\n",
      "Train Step: 1 (64.00%)  \tLoss: 0.0847\n",
      "Train Step: 1 (85.33%)  \tLoss: 0.3010\n",
      "Test Set: Average loss:0.1240,Accuracy:9614/10000(96.14%)\n",
      "\n",
      "Train Step: 2 (00.00%)  \tLoss: 0.0360\n",
      "Train Step: 2 (21.33%)  \tLoss: 0.1531\n",
      "Train Step: 2 (42.67%)  \tLoss: 0.0575\n",
      "Train Step: 2 (64.00%)  \tLoss: 0.1345\n",
      "Train Step: 2 (85.33%)  \tLoss: 0.0898\n",
      "Test Set: Average loss:0.0857,Accuracy:9729/10000(97.29%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 3 (00.00%)  \tLoss: 0.0286\n",
      "Train Step: 3 (21.33%)  \tLoss: 0.0521\n",
      "Train Step: 3 (42.67%)  \tLoss: 0.0452\n",
      "Train Step: 3 (64.00%)  \tLoss: 0.0226\n",
      "Train Step: 3 (85.33%)  \tLoss: 0.0026\n",
      "Test Set: Average loss:0.0882,Accuracy:9742/10000(97.42%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 4 (00.00%)  \tLoss: 0.0849\n",
      "Train Step: 4 (21.33%)  \tLoss: 0.0831\n",
      "Train Step: 4 (42.67%)  \tLoss: 0.0506\n",
      "Train Step: 4 (64.00%)  \tLoss: 0.2227\n",
      "Train Step: 4 (85.33%)  \tLoss: 0.0091\n",
      "Test Set: Average loss:0.0760,Accuracy:9784/10000(97.84%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 5 (00.00%)  \tLoss: 0.0401\n",
      "Train Step: 5 (21.33%)  \tLoss: 0.0182\n",
      "Train Step: 5 (42.67%)  \tLoss: 0.0442\n",
      "Train Step: 5 (64.00%)  \tLoss: 0.0053\n",
      "Train Step: 5 (85.33%)  \tLoss: 0.0683\n",
      "Test Set: Average loss:0.0857,Accuracy:9744/10000(97.44%)\n",
      "\n",
      "Train Step: 6 (00.00%)  \tLoss: 0.0209\n",
      "Train Step: 6 (21.33%)  \tLoss: 0.0080\n",
      "Train Step: 6 (42.67%)  \tLoss: 0.0081\n",
      "Train Step: 6 (64.00%)  \tLoss: 0.0551\n",
      "Train Step: 6 (85.33%)  \tLoss: 0.0012\n",
      "Test Set: Average loss:0.0836,Accuracy:9758/10000(97.58%)\n",
      "\n",
      "Train Step: 7 (00.00%)  \tLoss: 0.0045\n",
      "Train Step: 7 (21.33%)  \tLoss: 0.0254\n",
      "Train Step: 7 (42.67%)  \tLoss: 0.0553\n",
      "Train Step: 7 (64.00%)  \tLoss: 0.0495\n",
      "Train Step: 7 (85.33%)  \tLoss: 0.0604\n",
      "Test Set: Average loss:0.0827,Accuracy:9770/10000(97.70%)\n",
      "\n",
      "Train Step: 8 (00.00%)  \tLoss: 0.0041\n",
      "Train Step: 8 (21.33%)  \tLoss: 0.0222\n",
      "Train Step: 8 (42.67%)  \tLoss: 0.0006\n",
      "Train Step: 8 (64.00%)  \tLoss: 0.0150\n",
      "Train Step: 8 (85.33%)  \tLoss: 0.0048\n",
      "Test Set: Average loss:0.0828,Accuracy:9775/10000(97.75%)\n",
      "\n",
      "Train Step: 9 (00.00%)  \tLoss: 0.0153\n",
      "Train Step: 9 (21.33%)  \tLoss: 0.0010\n",
      "Train Step: 9 (42.67%)  \tLoss: 0.0282\n",
      "Train Step: 9 (64.00%)  \tLoss: 0.0026\n",
      "Train Step: 9 (85.33%)  \tLoss: 0.0010\n",
      "Test Set: Average loss:0.0904,Accuracy:9780/10000(97.80%)\n",
      "\n",
      "Train Step: 10 (00.00%)  \tLoss: 0.0787\n",
      "Train Step: 10 (21.33%)  \tLoss: 0.0438\n",
      "Train Step: 10 (42.67%)  \tLoss: 0.0004\n",
      "Train Step: 10 (64.00%)  \tLoss: 0.0043\n",
      "Train Step: 10 (85.33%)  \tLoss: 0.0011\n",
      "Test Set: Average loss:0.0922,Accuracy:9764/10000(97.64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(model=model, \n",
    "     train_loader=train_loader, \n",
    "     test_loader=test_loader, \n",
    "     loss_func=loss_function, \n",
    "     optimizer=optimizer, \n",
    "     n_step=STEP,\n",
    "     device=device,\n",
    "     save_path=\"mnist_model.pt\", \n",
    "     print_step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9467ed2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved ./figs/3.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = drawing_custom_number(preprocess=True,return_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93106618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted number is 3.\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.Tensor(np.array(img)).unsqueeze(0).to(device)\n",
    "pred = model(test_input)\n",
    "print(\"predicted number is {}.\".format(pred.softmax(1).argmax().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0ed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c10c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
